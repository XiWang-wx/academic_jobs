{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "academic_jobs_wiki_mining.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVaaOPETcIyr"
      },
      "source": [
        "# Academic Jobs Wiki"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCFiLppeE4AQ"
      },
      "source": [
        "import requests\n",
        "import bs4\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "from IPython.core.display import Pretty\n",
        "from datetime import datetime\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLHMsjaYDtLj"
      },
      "source": [
        "# use beautiful soup to retrieve the webpage\n",
        "wiki_URL = \"https://academicjobs.wikia.org/wiki/Comparative_2017\"\n",
        "wiki_page = requests.get(wiki_URL)\n",
        "wiki_soup = BeautifulSoup(wiki_page.text, \"html.parser\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfjU9WvME-jz"
      },
      "source": [
        "#retrieve job titles and job descriptions\n",
        "job_list = []\n",
        "\n",
        "for i in wiki_soup.find_all(name='h3')[:-1]:\n",
        "  job_list.append(i.text)\n",
        "  p = i.find_next(name='p')\n",
        "\n",
        "  while p.name == 'p':\n",
        "    job_list.append(p.text.replace('\\n',''))\n",
        "    p = p.next_sibling\n",
        "\n",
        "job_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm3_Tj28hNd5"
      },
      "source": [
        "# remove extra lines at the beginning\n",
        "del job_list[0:1]\n",
        "\n",
        "# transform all texts to lowercase\n",
        "job_list = [x.lower() for x in job_list]\n",
        "\n",
        "# remove extra lines at the end of each posts\n",
        "job_list = [x for x in job_list if not x.startswith('deadline') and not x.startswith('requeast') and not x.startswith('acknowledgment') and not x.startswith('rejection') and not x.startswith('offer') and not x.startswith('preliminary') and not x.startswith('campus')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-paem-qSlKi8"
      },
      "source": [
        "# create posts from title and description\n",
        "job_des = ''\n",
        "job_des = job_des.join(job_list)\n",
        "job_des = job_des.split('[edit | edit source]')\n",
        "\n",
        "job_no = len(job_des)\n",
        "\n",
        "# change the folder name and create a directory ('/content/...' in Google Colab)\n",
        "folder_name = 'comp_lit_16-17'\n",
        "path = f\"/content/{folder_name}\"\n",
        "os.mkdir(path)\n",
        "\n",
        "# create a list of txt files for each post\n",
        "for i in job_des:\n",
        "  job_no = job_des.index(i)\n",
        "  with open(f\"{folder_name}/{folder_name}_{job_no}.txt\",\"w\") as file:\n",
        "    file.write(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvJylX9UF0BM"
      },
      "source": [
        "# zip all files in the folder (update 'comp_lit_16-17' with new name and directory)\n",
        "!zip -r /content/comp_lit_16-17.zip /content/comp_lit_16-17"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBZWUssoIyHU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}